import os
import time
import traceback
import numpy as np
import pyaudio
from collections import deque
from funasr import AutoModel

MODEL_DIR = "./models/iic/"
# å¯¼å…¥é…ç½®å’Œå·¥å…·
from config import (
    SAMPLE_RATE, FORMAT, CHANNELS, 
    VAD_CHUNK_SIZE, ASR_CHUNK_SIZE, VAD_CHUNK_DURATION_MS,
    SIMILARITY_THRESHOLD, TEMP_WAV_PATH, TEACHER_WAV_PATH,
    COMMAND_KEYWORDS_STOP, COMMAND_KEYWORDS_START
)
from speaker_manager import SpeakerManager
from utils import detect_command, check_for_commands, save_temp_wav, register_teacher_from_file

class AudioStream:
    """éŸ³é¢‘æµåŸºç±»ï¼Œæ‰€æœ‰éŸ³é¢‘è¾“å…¥æºåº”ç»§æ‰¿æ­¤ç±»"""
    def read(self, size):
        """è¯»å–æŒ‡å®šå¤§å°çš„éŸ³é¢‘æ•°æ®"""
        raise NotImplementedError
    
    def close(self):
        """å…³é—­éŸ³é¢‘æµ"""
        pass

class MicrophoneStream(AudioStream):
    """éº¦å…‹é£éŸ³é¢‘æµå®ç°"""
    def __init__(self):
        self.p = pyaudio.PyAudio()
        self.stream = self.p.open(
            format=FORMAT,
            channels=CHANNELS,
            rate=SAMPLE_RATE,
            input=True,
            frames_per_buffer=VAD_CHUNK_SIZE
        )
    
    def read(self, size):
        return self.stream.read(size)
    
    def close(self):
        if hasattr(self, 'stream') and self.stream:
            self.stream.stop_stream()
            self.stream.close()
        if hasattr(self, 'p') and self.p:
            self.p.terminate()

class RecognitionState:
    """ç®¡ç†è¯­éŸ³è¯†åˆ«çš„çŠ¶æ€"""
    def __init__(self, dialog_mode: bool = False):
        self.vad_cache = {}
        self.asr_cache = {}
        self.asr_buffer = bytearray()
        self.spk_buffer = []
        self.pre_buffer = deque(maxlen=3)
        self.is_speaking = False
        self.current_speaker = "[è¯†åˆ«ä¸­]"
        self.is_speaker_identified = False
        self.current_sentence_text = ""
        self.last_asr_text = ""
        self.last_line_len = 0
        self.last_voice_time = time.time()
        self.asr_chunk_size = [0, 10, 5]
        self.encoder_chunk_look_back = 4
        self.decoder_chunk_look_back = 1
        self.dialog_mode = dialog_mode  # æ˜¯å¦å¯ç”¨â€œå¼€å§‹/åœæ­¢â€æŒ‡ä»¤æ¨¡å¼
        self.session_started = not dialog_mode  # æ™®é€š ASR ç›´æ¥å¼€å§‹
        self.pending_stop_command = None  # è®°å½•å¾…å¤„ç†çš„åœæ­¢å‘½ä»¤
        self.stop_command_processed = False  # æ ‡è®°æ˜¯å¦å·²å¤„ç†åœæ­¢å‘½ä»¤

    def reset_for_new_sentence(self):
        """é‡ç½®çŠ¶æ€ä»¥å¼€å§‹æ–°å¥å­"""
        self.asr_cache = {}
        self.asr_buffer = bytearray()
        self.spk_buffer = []
        self.current_speaker = "[è¯†åˆ«ä¸­]"
        self.is_speaker_identified = False
        self.current_sentence_text = ""
        self.last_asr_text = ""
        self.last_line_len = 0
        self.pending_stop_command = None
        self.stop_command_processed = False

class RealtimeAssistant:
    def __init__(self):
        self.model_asr = None
        self.model_vad = None
        self.model_spk = None
        self.model_punc = None
        self.speaker_mgr = None
        self.all_results = []
        self.stop_requested = False
        self.stop_requested_by_role = None
        self.dialog_mode = False  # è¿è¡Œæ—¶æ¨¡å¼ï¼šTrue=å¯¹è¯/è¯¾å ‚æŒ‡ä»¤æ¨¡å¼
        self._init_models()
        self._init_speaker_manager()

    def _init_models(self):
        """åˆå§‹åŒ–æ‰€æœ‰AIæ¨¡å‹"""
        print("æ­£åœ¨åŠ è½½æ¨¡å‹ï¼Œè¯·ç¨å€™...")
        try:
            print("æ­£åœ¨åŠ è½½è¯­éŸ³è¯†åˆ«æ¨¡å‹...")
            self.model_asr = AutoModel(
                model="paraformer-zh-streaming",
                # model=MODEL_DIR + "speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-online",
                model_revision="v2.0.4",
                disable_update=True
            )
            
            print("æ­£åœ¨åŠ è½½è¯­éŸ³æ£€æµ‹æ¨¡å‹...")
            self.model_vad = AutoModel(
                model="fsmn-vad",
                # model=MODEL_DIR + "speech_fsmn_vad_zh-cn-16k-common-pytorch",
                model_revision="v2.0.4",
                disable_update=True
            )
            
            print("æ­£åœ¨åŠ è½½å£°çº¹è¯†åˆ«æ¨¡å‹...")
            self.model_spk = AutoModel(
                model="cam++",
                # model=MODEL_DIR + "speech_campplus_sv_zh-cn_16k-common",
                model_revision="v2.0.2",
                disable_update=True
            )
            
            print("æ­£åœ¨åŠ è½½æ ‡ç‚¹ç¬¦å·æ¢å¤æ¨¡å‹...")
            self.model_punc = AutoModel(
                model="ct-punc",
                # model=MODEL_DIR + "punc_ct-transformer_cn-en-common-vocab471067-large",
                model_revision="v2.0.4",
                disable_update=True
            )
            print("æ‰€æœ‰æ¨¡å‹åŠ è½½å®Œæˆï¼")
        except Exception as e:
            print(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise e

    def _init_speaker_manager(self):
        """åˆå§‹åŒ–å£°çº¹ç®¡ç†å™¨å¹¶æ³¨å†Œè€å¸ˆ"""
        self.speaker_mgr = SpeakerManager(threshold=SIMILARITY_THRESHOLD)

        if not self.speaker_mgr.teacher_embeddings:
            print("æ£€æµ‹åˆ°å°šæœªæ³¨å†Œè€å¸ˆå£°çº¹ã€‚")
            if os.path.exists(TEACHER_WAV_PATH):
                print(f"å‘ç°é¢„ç½®éŸ³é¢‘æ–‡ä»¶: {TEACHER_WAV_PATH}")
                register_teacher_from_file(self.model_spk, self.speaker_mgr, TEACHER_WAV_PATH)
            else:
                print(f"è­¦å‘Š: æœªæ‰¾åˆ°éŸ³é¢‘æ–‡ä»¶ {TEACHER_WAV_PATH}")
                print("æ— æ³•æ³¨å†Œè€å¸ˆå£°çº¹ã€‚æ‰€æœ‰è¯´è¯äººå°†è¢«è¯†åˆ«ä¸ºå­¦ç”Ÿã€‚")
        else:
            print(f"å·²åŠ è½½è€å¸ˆå£°çº¹: [{self.speaker_mgr.teacher_name}]")
            print(">>> ç›´æ¥è¿›å…¥å®æ—¶åŠ©æ‰‹æ¨¡å¼ <<<")

    def get_text_width(self, text):
        """è®¡ç®—æ–‡æœ¬çš„æ˜¾ç¤ºå®½åº¦ (ä¸­æ–‡å­—ç¬¦è®¡ä¸º2ï¼Œå…¶ä»–è®¡ä¸º1)"""
        return sum(2 if '\u4e00' <= char <= '\u9fff' else 1 for char in text)

    def _add_punctuation(self, text):
        """æ·»åŠ æ ‡ç‚¹ç¬¦å·ï¼Œä¼˜å…ˆä½¿ç”¨æ¨¡å‹ï¼Œå¤±è´¥æ—¶ä½¿ç”¨ç®€å•åå¤„ç†"""
        if not text.strip() or self.model_punc is None:
            return text
        
        try:
            result = self.model_punc.generate(text, disable_pbar=True)
            if result and len(result) > 0 and 'text' in result[0]:
                return result[0]['text']
        except Exception as e:
            print(f"æ ‡ç‚¹ç¬¦å·æ¢å¤å¤±è´¥: {e}")
            traceback.print_exc()
        
        return self._simple_punctuation(text)

    def _simple_punctuation(self, text):
        """ç®€å•çš„æ ‡ç‚¹ç¬¦å·åå¤„ç†"""
        if not text.strip():
            return text
        
        text = text.strip()
        sentence_enders = ['å—', 'å‘¢', 'å§', 'å•Š', 'å‘€', 'å•¦', 'å“¦', 'å“ˆ', 'å—¯', 'å¥½', 'æ˜¯', 'å¯¹', 'é”™', 'è¡Œ', 'å¯ä»¥', 'ä¸è¡Œ', 'çŸ¥é“', 'æ˜ç™½', 'ç†è§£', 'åŒæ„', 'åå¯¹']
        
        if any(text.endswith(ender) for ender in ['å—', 'å‘¢', 'å§']):
            if not text.endswith(('ï¼Ÿ', '?')):
                text += 'ï¼Ÿ'
        elif text.endswith(('ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?')):
            return text
        elif any(punc in text for punc in ['ï¼Œ', 'ã€', 'ï¼š', 'ï¼›', 'ï¼ˆ', 'ï¼‰', 'â€œ', 'â€', 'ã€', 'ã€‘']):
            return text
        else:
            text += 'ã€‚'
        
        return text

    def _is_teacher_speaker(self, speaker):
        """æ£€æŸ¥è¯´è¯äººæ˜¯å¦æ˜¯è€å¸ˆ"""
        return speaker and speaker not in ["[è¯†åˆ«ä¸­]", "[Unknown]"] and "Teacher" in speaker

    def _get_speaker_role(self, speaker):
        if self._is_teacher_speaker(speaker):
            return "teacher"
        if not speaker or "Unknown" in speaker:
            return "unknown"
        return "student"

    def _is_authorized(self, role, command_match):
        if not command_match:
            return False
        roles = command_match.get("roles", [])
        if not roles:
            return True
        return role in roles

    def _match_command(self, text):
        if not self.dialog_mode:
            return None
        return detect_command(text)

    def _check_stop_command(self, text):
        """æ£€æŸ¥æ–‡æœ¬ä¸­æ˜¯å¦åŒ…å«åœæ­¢å‘½ä»¤"""
        if not self.dialog_mode:
            return None
        cmd = check_for_commands(text)
        stop_commands = COMMAND_KEYWORDS_STOP
        return cmd if cmd in stop_commands else None

    def _check_start_command(self, text):
        """æ£€æŸ¥æ–‡æœ¬ä¸­æ˜¯å¦åŒ…å«å¼€å§‹/ä¸Šè¯¾å‘½ä»¤"""
        if not self.dialog_mode:
            return None
        cmd = check_for_commands(text)
        start_commands = COMMAND_KEYWORDS_START
        if cmd in start_commands:
            return cmd
        # ç®€å•æ¨¡ç³ŠåŒ¹é…ï¼šå¦‚æœåŒ…å«"ä¸Šè¯¾"ä¸¤ä¸ªå­—
        if "ä¸Šè¯¾" in text:
            return "ä¸Šè¯¾"
        return None

    def _save_final_result_with_stop_command(self, speaker, text, is_teacher):
        """ä¿å­˜åŒ…å«åœæ­¢å‘½ä»¤çš„ç»“æœ"""
        if not text.strip():
            return
            
        punctuated_text = self._add_punctuation(text.strip())
        result = {
            'speaker': speaker,
            'text': punctuated_text,
            'raw_text': text.strip(),
            'timestamp': time.time(),
            'contains_stop_command': True,
            'triggered_by_teacher': is_teacher
        }
        
        if not is_teacher:
            result['ignored_stop_command'] = True
            
        self.all_results.append(result)
        print(f"\nâœ… ä¿å­˜åŒ…å«åœæ­¢å‘½ä»¤çš„å¥å­ ({'è€å¸ˆ' if is_teacher else 'å­¦ç”Ÿ'}): {speaker}: {punctuated_text}")

    def _save_final_result(self, speaker, text):
        """ä¿å­˜æœ€ç»ˆè¯†åˆ«ç»“æœ"""
        if not text or not text.strip():
            return None
            
        cmd_match = self._match_command(text)
        if cmd_match and cmd_match.get("type") == "stop":
            return None
            
        punctuated_text = self._add_punctuation(text.strip())
        
        result = {
            'speaker': speaker,
            'text': punctuated_text,
            'raw_text': text.strip(),
            'timestamp': time.time()
        }
        self.all_results.append(result)
        print(f"\nâœ… ä¿å­˜è¯†åˆ«ç»“æœ: {speaker}: {punctuated_text}")
        return result

    def _process_vad_result(self, audio_chunk_np, state):
        """å¤„ç†VADç»“æœå¹¶æ›´æ–°çŠ¶æ€"""
        try:
            res_vad = self.model_vad.generate(
                input=audio_chunk_np, 
                cache=state.vad_cache, 
                is_final=False, 
                chunk_size=VAD_CHUNK_DURATION_MS,
                disable_pbar=True
            )
            
            vad_segments = res_vad[0]['value'] if res_vad else []
            
            for segment in vad_segments:
                if segment[0] != -1:
                    # è¯­éŸ³å¼€å§‹
                    state.is_speaking = True
                    self._prepend_pre_buffer_audio(state)
                    self._print_new_line_header(state) # ä¼ å…¥ state å¯¹è±¡
                
                if segment[1] != -1:
                    # è¯­éŸ³ç»“æŸ
                    self._handle_speech_end(state)
                    
        except Exception as e:
            print(f"\nVADå¤„ç†é”™è¯¯: {e}")
            traceback.print_exc()

    def _prepend_pre_buffer_audio(self, state):
        """å°†é¢„å½•åˆ¶ç¼“å†²åŒºçš„éŸ³é¢‘åŠ å…¥å¤„ç†ç¼“å†²åŒº"""
        for chunk in state.pre_buffer:
            state.asr_buffer.extend(chunk)
            state.spk_buffer.append(np.frombuffer(chunk, dtype=np.int16))

    def _print_new_line_header(self, state):
        """æ‰“å°æ–°è¡Œå¤´"""
        if not state.session_started:
            return 0
        line_content = f"{state.current_speaker}: "
        print(f"\r{line_content}", end="", flush=True)
        return self.get_text_width(line_content)

    def _handle_sentence_completion(self, state, final_text):
        """
        å¤„ç†å¥å­å®Œæˆï¼Œç»Ÿä¸€è¿›è¡Œæƒé™æ£€æŸ¥å’Œä¿å­˜
        Returns: bool - æ˜¯å¦éœ€è¦åœæ­¢è¯†åˆ«
        """
        if not final_text.strip():
            return False
            
        punctuated_text = self._add_punctuation(final_text.strip())
        role = self._get_speaker_role(state.current_speaker)
        is_teacher = (role == "teacher")
        
        # === [æ–°å¢é€»è¾‘] æ£€æŸ¥æ˜¯å¦è¿˜æœªå¼€å§‹ä¸Šè¯¾ ===
        if not state.session_started:
            start_cmd = self._match_command(final_text)
            
            # åªæœ‰è€å¸ˆè¯´â€œä¸Šè¯¾â€æ‰æœ‰æ•ˆ (å¦‚æœè°ƒè¯•æ¨¡å¼ä¸‹å¼ºåˆ¶Teacherï¼Œè¿™é‡Œè‡ªç„¶ä¼šè¿‡)
            if start_cmd and start_cmd.get("type") == "start" and self._is_authorized(role, start_cmd):
                state.session_started = True  # æ ‡è®°ä¸ºå·²å¼€å§‹
                
                # ä¿å­˜è¿™å¥è¯ï¼ˆä½œä¸ºç¬¬ä¸€å¥ï¼‰
                result = {
                    'speaker': state.current_speaker,
                    'text': punctuated_text,
                    'raw_text': final_text.strip(),
                    'timestamp': time.time()
                }
                self.all_results.append(result)
                print(f"\nğŸ””  [{state.current_speaker}] å®£å¸ƒä¸Šè¯¾ï¼Œå¼€å§‹æ­£å¼è®°å½•ä¼šè®®å†…å®¹...")
                print(f"âœ… ä¿å­˜ä¸Šè¯¾æŒ‡ä»¤: {state.current_speaker}: {punctuated_text}")
                return False
            else:
                # è¿˜æ²¡å¼€å§‹ä¸Šè¯¾ï¼Œå¿½ç•¥è¿™å¥è¯
                # print(f"\nğŸ’¤  (æœªä¸Šè¯¾) å¿½ç•¥: {state.current_speaker}: {punctuated_text}")
                return False
        
        # === ä»¥ä¸‹æ˜¯åŸæœ‰çš„é€»è¾‘ï¼ˆå·²å¼€å§‹ä¸Šè¯¾ï¼‰ ===
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«åœæ­¢å‘½ä»¤
        cmd_match = self._match_command(final_text)
        stop_command = cmd_match.get("keyword") if cmd_match else None
        
        # æ„å»ºç»“æœå¯¹è±¡
        result = {
            'speaker': state.current_speaker,
            'text': punctuated_text,
            'raw_text': final_text.strip(),
            'timestamp': time.time()
        }
        
        # å¤„ç†åœæ­¢å‘½ä»¤é€»è¾‘
        if cmd_match and cmd_match.get("type") == "stop":
            result['contains_stop_command'] = True
            result['triggered_by_teacher'] = is_teacher
            result['triggered_by_role'] = role
            authorized = self._is_authorized(role, cmd_match)
            result['authorized'] = authorized
            
            if authorized:
                self.stop_requested_by_role = role
                print(f"\nğŸ›‘ è€å¸ˆè¦æ±‚ä¸‹è¯¾: {stop_command}")
                # åŸæœ‰çš„ _save_final_result_with_stop_command é€»è¾‘ç°åœ¨è¢«ç®€åŒ–ä¸º append + return True
                self.all_results.append(result)
                print(">>> åœæ­¢è¯†åˆ«ã€‚")
                return True
            else:
                print(f"\nâ„¹ï¸  å­¦ç”Ÿè¯´ '{stop_command}'ï¼Œä½†åªæœ‰è€å¸ˆå¯ä»¥åœæ­¢è¯†åˆ«")
                self.all_results.append(result)
                return False

        # å¸¸è§„ä¿å­˜
        self.all_results.append(result)
        print(f"\nâœ… ä¿å­˜è¯†åˆ«ç»“æœ: {state.current_speaker}: {punctuated_text}")
        return False


    def _handle_speech_end(self, state):
        """å¤„ç†è¯­éŸ³ç»“æŸ - é‡æ„ç‰ˆæœ¬"""
        state.is_speaking = False
        final_speaker = state.current_speaker
        final_text = ""
        
        try:
            if len(state.asr_buffer) > 0:
                asr_chunk_np = np.frombuffer(state.asr_buffer, dtype=np.int16)
                res_asr = self.model_asr.generate(
                    input=asr_chunk_np, 
                    cache=state.asr_cache, 
                    is_final=True,
                    chunk_size=state.asr_chunk_size,
                    encoder_chunk_look_back=state.encoder_chunk_look_back, 
                    decoder_chunk_look_back=state.decoder_chunk_look_back,
                    disable_pbar=True
                )
                
                if res_asr:
                    text = res_asr[0]['text']
                    delta = text[len(state.last_asr_text):] if text.startswith(state.last_asr_text) else text
                    final_text = state.current_sentence_text + delta
            else:
                final_text = state.current_sentence_text
                
            # ç»Ÿä¸€å¤„ç†å¥å­å®Œæˆ
            if final_text.strip():
                should_stop = self._handle_sentence_completion(state, final_text)
                if should_stop:
                    self.stop_requested = True
                
                # [ä¿®æ”¹] åªæœ‰åœ¨ä¼šè®®å¼€å§‹åï¼Œæ‰æ‰“å° "å¥å­å®Œæˆ"
                if state.session_started:
                    print(f"\nğŸ“ å¥å­å®Œæˆ: {final_speaker}: {final_text}")
            else:
                print()  # æ²¡æœ‰å†…å®¹ï¼Œåªæ¢è¡Œ
                
        except Exception as e:
            print(f"\nASRå¤„ç†é”™è¯¯: {e}")
            traceback.print_exc()
            if state.current_sentence_text.strip():
                # å‡ºé”™æ—¶ä¹Ÿç»Ÿä¸€å¤„ç†
                should_stop = self._handle_sentence_completion(state, state.current_sentence_text)
                if should_stop:
                    self.stop_requested = True
                print(f"\nğŸ“ å¥å­å®Œæˆ (ASRé”™è¯¯): {final_speaker}: {state.current_sentence_text}")
        
        state.reset_for_new_sentence()


    def _process_asr_chunk(self, audio_chunk, state):
        """å¤„ç†ASRå— - ç§»é™¤å®æ—¶åœæ­¢å‘½ä»¤æ£€æŸ¥"""
        state.asr_buffer.extend(audio_chunk)
        
        if len(state.asr_buffer) >= ASR_CHUNK_SIZE * 2:
            chunk_bytes = state.asr_buffer[:ASR_CHUNK_SIZE * 2]
            state.asr_buffer = state.asr_buffer[ASR_CHUNK_SIZE * 2:]
            
            asr_chunk_np = np.frombuffer(chunk_bytes, dtype=np.int16)
            
            try:
                res_asr = self.model_asr.generate(
                    input=asr_chunk_np, 
                    cache=state.asr_cache, 
                    is_final=False, 
                    chunk_size=state.asr_chunk_size,
                    encoder_chunk_look_back=state.encoder_chunk_look_back, 
                    decoder_chunk_look_back=state.decoder_chunk_look_back,
                    disable_pbar=True
                )
                
                if res_asr:
                    text = res_asr[0]['text']
                    if text:
                        delta = text[len(state.last_asr_text):] if text.startswith(state.last_asr_text) else text
                        state.current_sentence_text += delta
                        state.last_asr_text = text
                        self._refresh_display_line(state)
                        
                        # ç§»é™¤å®æ—¶åœæ­¢å‘½ä»¤æ£€æŸ¥ - æ”¹ä¸ºåœ¨å¥å­ç»“æŸæ—¶ç»Ÿä¸€å¤„ç†
                        
            except Exception as e:
                print(f"\nASRå¤„ç†é”™è¯¯: {e}")
                traceback.print_exc()

    def _refresh_display_line(self, state):
        """åˆ·æ–°æ˜¾ç¤ºè¡Œ"""
        if not state.session_started:
            return
        line_content = f"{state.current_speaker}: {state.current_sentence_text}"
        current_width = self.get_text_width(line_content)
        padding_len = max(0, state.last_line_len - current_width + 4)
        padding = " " * padding_len
        
        print(f"\r{line_content}{padding}", end="", flush=True)
        state.last_line_len = current_width

    def _identify_speaker(self, state):
        """è¯†åˆ«è¯´è¯äººå£°çº¹"""
        # # ============== [è°ƒè¯•ä»£ç å¼€å§‹] ==============
        # # å¼ºåˆ¶å°†æ‰€æœ‰è¯´è¯äººè®¾ç½®ä¸º "Teacher"
        # state.current_speaker = "Teacher" 
        # state.is_speaker_identified = True
        # return # ç›´æ¥è¿”å›ï¼Œä¸æ‰§è¡Œåé¢çœŸæ­£çš„AIè¯†åˆ«
        # # ============== [è°ƒè¯•ä»£ç ç»“æŸ] ==============
        if state.is_speaker_identified or len(state.spk_buffer) < 6:
            return
            
        full_audio = np.concatenate(state.spk_buffer)
        save_temp_wav(full_audio, SAMPLE_RATE, TEMP_WAV_PATH)
        
        try:
            spk_res = self.model_spk.generate(TEMP_WAV_PATH, disable_pbar=True)
            if spk_res and len(spk_res) > 0 and 'spk_embedding' in spk_res[0]:
                emb = spk_res[0]['spk_embedding']
 # ä¿®å¤CUDAå¼ é‡è½¬æ¢é—®é¢˜ï¼šå°†è®¾å¤‡ä¸Šçš„å¼ é‡ç§»è‡³CPU
                if hasattr(emb, 'cpu'):
                    emb = emb.cpu().numpy()
                new_speaker = self.speaker_mgr.identify(emb)
                
                if new_speaker != state.current_speaker:
                    state.current_speaker = new_speaker
                    self._refresh_display_line(state)
        except Exception as e:
            print(f"\nå£°çº¹è¯†åˆ«é”™è¯¯: {e}")
            traceback.print_exc()
            state.current_speaker = "[Unknown]"
        finally:
            if os.path.exists(TEMP_WAV_PATH):
                os.remove(TEMP_WAV_PATH)
        
        state.is_speaker_identified = True
        return

    def _process_remaining_audio(self, state):
        """å¤„ç†å‰©ä½™éŸ³é¢‘æ•°æ® - å¢å¼ºç‰ˆï¼Œç¡®ä¿ä¸ä¸¢å¤±å·²è¯†åˆ«æ–‡æœ¬"""
        if self.stop_requested or not state.is_speaking:
            return
            
        print("\nğŸ”„ å¤„ç†å‰©ä½™éŸ³é¢‘æ•°æ®...")
        
        # ä¿æŠ¤æ€§æ£€æŸ¥ï¼šå³ä½¿ASRå¤„ç†å¤±è´¥ï¼Œä¹Ÿè¦ä¿å­˜å·²ç´¯ç§¯çš„æ–‡æœ¬
        try:
            if len(state.asr_buffer) > 0:
                asr_chunk_np = np.frombuffer(state.asr_buffer, dtype=np.int16)
                res_asr = self.model_asr.generate(
                    input=asr_chunk_np, 
                    cache=state.asr_cache, 
                    is_final=True,
                    chunk_size=state.asr_chunk_size,
                    encoder_chunk_look_back=state.encoder_chunk_look_back, 
                    decoder_chunk_look_back=state.decoder_chunk_look_back,
                    disable_pbar=True
                )
                
                if res_asr:
                    text = res_asr[0]['text']
                    if text.strip():
                        final_text = state.current_sentence_text + text
                        # æ£€æŸ¥åœæ­¢å‘½ä»¤å¹¶ä¿å­˜
                        should_stop = self._handle_sentence_completion(state, final_text)
                        if should_stop:
                            self.stop_requested = True
                        print(f"\nğŸ“ å¤„ç†å®Œæˆ: {state.current_speaker}: {final_text}")
                        return  # æ­£å¸¸å¤„ç†å®Œæˆï¼Œç›´æ¥è¿”å›
        except Exception as e:
            print(f"\nå‰©ä½™éŸ³é¢‘å¤„ç†é”™è¯¯: {e}")
            traceback.print_exc()
        
        # Fallbackæœºåˆ¶ï¼šå¦‚æœASRå¤„ç†å¤±è´¥ï¼Œä¿å­˜å·²ç´¯ç§¯çš„æ–‡æœ¬
        if state.current_sentence_text.strip() and not self.stop_requested:
            should_stop = self._handle_sentence_completion(state, state.current_sentence_text)
            if should_stop:
                self.stop_requested = True
            print(f"\nâš ï¸  Fallback: ä¿å­˜å·²ç´¯ç§¯æ–‡æœ¬ (ASRå¤„ç†å¤±è´¥): {state.current_speaker}: {state.current_sentence_text}")
    def run_stream(self, audio_stream, timeout=30, mode="plain"):
        """
        æµå¼å¤„ç†éŸ³é¢‘è¾“å…¥ - é‡æ„ç‰ˆæœ¬
        Args:
            audio_stream: ç”Ÿæˆ16bit pcméŸ³é¢‘æ•°æ®çš„ç”Ÿæˆå™¨
            timeout: æ— è¯­éŸ³è¾“å…¥æ—¶çš„è¶…æ—¶æ—¶é—´(ç§’)
            mode: æ¨¡å¼é€‰æ‹©ï¼Œ"plain"=æ™®é€šASRï¼Œ"dialog"=å¯ç”¨å¼€å§‹/åœæ­¢æŒ‡ä»¤
        Returns:
            list: æ‰€æœ‰è¯†åˆ«ç»“æœ
        """
        dialog_mode = (mode == "dialog")
        self.dialog_mode = dialog_mode  # ä¿å­˜å½“å‰ä¼šè¯æ¨¡å¼ï¼ˆå½±å“æŒ‡ä»¤å¤„ç†ï¼‰

        print("\n" + "="*50)
        print("  æµå¼è¯­éŸ³è¯†åˆ«æ¨¡å¼å·²å¯åŠ¨...")
        print("  ç­‰å¾…éŸ³é¢‘æ•°æ®è¾“å…¥...")
        if dialog_mode:
            print("  ã€æ³¨æ„ã€‘è¯·è€å¸ˆå…ˆè¯´ â€œä¸Šè¯¾â€ æˆ– â€œå¼€å§‹ä¸Šè¯¾â€ æ¥æ¿€æ´»è®°å½•ï¼")
            print("  åªæœ‰è€å¸ˆå¯ä»¥è¯´'ä¸‹è¯¾'æˆ–'åœæ­¢è®°å½•'æ¥ç»“æŸè¯†åˆ«")
        else:
            print("  ã€æ³¨æ„ã€‘æ™®é€š ASR æ¨¡å¼ï¼Œæ— éœ€â€œä¸Šè¯¾/ä¸‹è¯¾â€æŒ‡ä»¤")
        print("="*50)
        
        # é‡ç½®çŠ¶æ€
        self.all_results = []
        self.stop_requested = False
        self.stop_requested_by_role = None
        state = RecognitionState(dialog_mode=dialog_mode)
        
        try:
            for audio_chunk in audio_stream:
                if len(audio_chunk) == 0:
                    continue
                
                state.last_voice_time = time.time()
                audio_chunk_np = np.frombuffer(audio_chunk, dtype=np.int16)
                
                # å¤„ç†VAD
                self._process_vad_result(audio_chunk_np, state)
                
                # å¤„ç†æ­£åœ¨è¯´è¯çš„æƒ…å†µ
                if state.is_speaking:
                    self._process_asr_chunk(audio_chunk, state)
                    if not state.is_speaker_identified:
                        state.spk_buffer.append(audio_chunk_np)
                        self._identify_speaker(state)
                
                # æ£€æŸ¥åœæ­¢å‘½ä»¤
                if self.stop_requested:
                    print("\nâ¹ï¸  è€å¸ˆæŒ‡ä»¤ï¼Œç»“æŸè¯†åˆ«...")
                    break
                
                # æ›´æ–°é¢„å½•åˆ¶ç¼“å†²åŒº
                state.pre_buffer.append(audio_chunk)
                
                # æ£€æŸ¥è¶…æ—¶
                if time.time() - state.last_voice_time > timeout and not state.is_speaking:
                    print(f"\nâ° è¶…æ—¶ ({timeout}ç§’æ— è¾“å…¥)ï¼Œåœæ­¢å¤„ç†...")
                    break
            
            # å¤„ç†å‰©ä½™æ•°æ®
            self._process_remaining_audio(state)
            
            print(f"\nâœ… è¯†åˆ«å®Œæˆï¼Œå…±è¯†åˆ«åˆ° {len(self.all_results)} ä¸ªå¥å­")
            return self.all_results
            
        except KeyboardInterrupt:
            print("\nâ¹ï¸  ç”¨æˆ·ä¸­æ–­è¯†åˆ«...")
            if state.current_sentence_text.strip() and not self.stop_requested:
                self._save_final_result(state.current_speaker, state.current_sentence_text)
            return self.all_results
        except Exception as e:
            print(f"\nâŒ å¤„ç†é”™è¯¯: {e}")
            traceback.print_exc()
            if state.current_sentence_text.strip() and not self.stop_requested:
                self._save_final_result(state.current_speaker, state.current_sentence_text)
            return self.all_results

    def run(self, mode="plain"):
        """å…¼å®¹æ€§æ–¹æ³•ï¼Œä½¿ç”¨éº¦å…‹é£æµ"""
        return self.run_stream(MicrophoneStream(), mode=mode)

def main():
    assistant = RealtimeAssistant()
    results = assistant.run()
    if results:
        print("\n=== æ‰€æœ‰è¯†åˆ«ç»“æœ ===")
        for i, result in enumerate(results, 1):
            if 'contains_stop_command' in result:
                if result.get('triggered_by_teacher', False):
                    print(f"{i}. {result['speaker']}: {result['text']} (è€å¸ˆè§¦å‘åœæ­¢)")
                else:
                    print(f"{i}. {result['speaker']}: {result['text']} (å­¦ç”Ÿè¯´åœæ­¢å‘½ä»¤ï¼Œå·²å¿½ç•¥)")
            else:
                print(f"{i}. {result['speaker']}: {result['text']}")

if __name__ == "__main__":
    main()
